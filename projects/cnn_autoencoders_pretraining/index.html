<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Pretraining of deep CNN autoencoders | Tomáš Čelko </title> <meta name="author" content="Tomáš Čelko"> <meta name="description" content="Autoencoders pretrained on unlabeled data from Timepix3 to improve accuracy."> <meta name="keywords" content="celko cluster clustering"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tomascelko.github.io/projects/cnn_autoencoders_pretraining/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tomáš</span> Čelko </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Pretraining of deep CNN autoencoders</h1> <p class="post-description">Autoencoders pretrained on unlabeled data from Timepix3 to improve accuracy.</p> </header> <article> <h2 id="goal">Goal</h2> <p>The goal of the project is to show how pretraining autoencoders based on CNNs can be used to learn feature extraction and subsequently finetune for specific task. This page was made to showcase some of the figures relevant to the project.</p> <h2 id="model">Model</h2> <p>We have chosen the EfficientNetB4 as our model architecture of the encoder. For the decoder, we transpose the architecture of the encoder. Additionally, we attach multiple heads to the model -</p> <ul> <li>First head predicts the image on the input</li> <li>Second head predicts binary mask of the whole cluster</li> <li>Third head predict the binary mask of pixels above fixed threashold</li> </ul> <h3 id="variations">Variations</h3> <ul> <li>Different levels of input pixel masking were tried (0%, 25% and 50%)</li> <li>Variations with or without binary mask head were tested</li> </ul> <h2 id="input">Input</h2> <p>As input to the model we use 3 channel image of size 384x384, which is upsampled image of the cluster (up to 5x size increase in each dimension).</p> <ul> <li>First chanel is dedicated to deposited energy for each pixel.</li> <li>Second channel is for the time of arrival of each pixel</li> <li>Third channel contains binary map of the pixels above certain energy threshold Data is normalized using sqare-root normalization.</li> </ul> <h2 id="training">Training</h2> <h3 id="pretraining">Pretraining</h3> <p>The pretraining has two stages :</p> <ol> <li> <p>Standard pretraining on Imagenet dataset - we simply download the pretrained weights from for this torchvision model.</p> </li> <li> <p>Part of the data for unsupervised pretraining is obtained from Timepix3 measurements in ATLAS. This dataset contains high variability of clusters but smaller clusters are much more frequent. This is the reason why we also used ions data obtained at test beam measurements of Pb at SPS at Cern - these have higher frequency of large clusters.</p> </li> </ol> <p>Dataset is balanced into exponentially sized bins based on cluster hit count. We then downsample the clusters so that the frequency across the bins is unfiorm.</p> <div class="row"> <figure class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/AE_CNN/curve-480.webp 480w,/assets/AE_CNN/curve-800.webp 800w,/assets/AE_CNN/curve-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/AE_CNN/curve.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Reconstruction of energy of a curly track using CNN autoencoder as a function of number of training iterations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="mt-2 text-muted"> Reconstruction of energy of a curly track using a CNN autoencoder as a function of training iterations. </figcaption> </figure> </div> <div class="row"> <figure class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/AE_CNN/curve2-480.webp 480w,/assets/AE_CNN/curve2-800.webp 800w,/assets/AE_CNN/curve2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/AE_CNN/curve2.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Reconstruction of energy of a curly track using CNN autoencoder as a function of number of training iterations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </figure> </div> <div class="row"> <figure class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/AE_CNN/hblob-480.webp 480w,/assets/AE_CNN/hblob-800.webp 800w,/assets/AE_CNN/hblob-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/AE_CNN/hblob.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Reconstruction of energy of ion track using CNN autoencoder as a function of number of training iterations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </figure> </div> <div class="row"> <figure class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/AE_CNN/hion-480.webp 480w,/assets/AE_CNN/hion-800.webp 800w,/assets/AE_CNN/hion-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/AE_CNN/hion.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Reconstruction of energy of ion track using CNN autoencoder as a function of number of training iterations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </figure> </div> <div class="row"> <figure class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/AE_CNN/htrack-480.webp 480w,/assets/AE_CNN/htrack-800.webp 800w,/assets/AE_CNN/htrack-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/AE_CNN/htrack.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Reconstruction of ion track using CNN autoencoder as a function of number of training iterations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </figure> </div> <h4 id="what-happens-if-we-do-not-use-binary-pixel-masking">What happens, if we do not use binary pixel masking?</h4> <p>Suddenly, the thin tracks become more blurry as we can see below.</p> <div class="row"> <figure class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/AE_CNN/curve_no_mask-480.webp 480w,/assets/AE_CNN/curve_no_mask-800.webp 800w,/assets/AE_CNN/curve_no_mask-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/AE_CNN/curve_no_mask.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Reconstruction of electron track using CNN autoencoder as a function of number of training iterations. This time, no binary mask predicition is used" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </figure> </div> <div class="row"> <figure class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/AE_CNN/htrack_no_mask-480.webp 480w,/assets/AE_CNN/htrack_no_mask-800.webp 800w,/assets/AE_CNN/htrack_no_mask-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/AE_CNN/htrack_no_mask.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Reconstruction of ion track using CNN autoencoder as a function of number of training iterations. This time, no binary mask predicition mask is used" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </figure> </div> <h4 id="key-observations">Key observations:</h4> <ul> <li>addition of binary mask predicition non-trivially increased the sharpness of the recontructed image</li> <li>we were able to reconstruct simple, linear tracks with high precision</li> <li>heavy ions were reconstructed too but reconstructing areas with high density of delta electrons ended up blurred</li> </ul> <h3 id="supervised-finetuning">Supervised finetuning</h3> <p>Data for supervised finetuning is obtained from simulations. For instance, we simulate protons and electrons for SATRAM experiment with Timepix1. Currently, there is a lack of standardized benchmarking datasets for this type of detectors, so besides simulation of real spectrum for SATRAM instrument, we also created identical spectrum to the one mentioned in this paper <a href="https://hal.science/hal-03238974v1/document" rel="external nofollow noopener" target="_blank"> </a>, further referred to as “reference spectrum” as opposed to the original “real spectrum”.</p> <div class="row"> <figure class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/AE_CNN/electron_real_spectrum-480.webp 480w,/assets/AE_CNN/electron_real_spectrum-800.webp 800w,/assets/AE_CNN/electron_real_spectrum-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/AE_CNN/electron_real_spectrum.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Simulated real SATRAM electron spectrum" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </figure> </div> <div class="row"> <figure class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/AE_CNN/proton_real_spectrum-480.webp 480w,/assets/AE_CNN/proton_real_spectrum-800.webp 800w,/assets/AE_CNN/proton_real_spectrum-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/AE_CNN/proton_real_spectrum.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Simulated real SATRAM proton spectrum" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </figure> </div> <h4 id="validation">Validation</h4> <div class="row"> <figure class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/AE_CNN/mask-480.webp 480w,/assets/AE_CNN/mask-800.webp 800w,/assets/AE_CNN/mask-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/AE_CNN/mask.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Validation accuracy for simulated SATRAM data during training" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="mt-2 text-muted"> On the plot we can see how validation accuracy improves over the course of training. We can observe that pixel masking does not seem to have any measurable influence on the classifier accuracy. </figcaption> </figure> </div> <div class="row"> <figure class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/AE_CNN/marie-480.webp 480w,/assets/AE_CNN/marie-800.webp 800w,/assets/AE_CNN/marie-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/AE_CNN/marie.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Validation accuracy for simulated SATRAM data during training" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="mt-2 text-muted"> n the plot we can see how validation accuracy improves over the course of training. The plot shows comparison of three methods - real spectrum as created in our simulations, reference spectrum recreated as shown in <a href="https://hal.science/hal-03238974v1/document" rel="external nofollow noopener" target="_blank"> </a> and its variant with tracks containing more than 8 pixles. The main observation is that the reference spectrum seems to be easier to separate than the real one and most of its errors come from clusters with less than 8 pixels. </figcaption> </figure> </div> <div class="row"> <figure class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/AE_CNN/training_gain-480.webp 480w,/assets/AE_CNN/training_gain-800.webp 800w,/assets/AE_CNN/training_gain-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/AE_CNN/training_gain.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Validation accuracy for simulated SATRAM data during training" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="mt-2 text-muted"> In this plot we can see the overall difference of non-pretrained efficient net model classification and the pretrained one. One can see that the overall validation accuracy for the real spectrum is slightly better for pretrained model, than the non-pretrained one. Even though the maximum achieved accuracy is not drastically better, the convergence speed difference between pretrained and non-pretrained model is evident. </figcaption> </figure> </div> <h4 id="separated-protons-and-electron-feature-vectors">Separated protons and electron feature vectors</h4> <div class="row"> <figure class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/AE_CNN/umap_mask0_after-480.webp 480w,/assets/AE_CNN/umap_mask0_after-800.webp 800w,/assets/AE_CNN/umap_mask0_after-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/AE_CNN/umap_mask0_after.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Projection of extracted features of each cluster to 2D using UMAP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="mt-2 text-muted"> Projection of feature vector obtained from CNN encoder into 2D AFTER unsupervised pretraining on ATLAS data and also AFTER supervised finetuning to split protons from electrons. Some tracks were selected (using K-means) and for these we also show energy deposition and the tracks shape in a square box. We can see that tracks with similar angle are close together indicating model learned the feature to recognize particle angle. Another extracted feature seems to be track length. This indicates that model is indeed building meaningful and interpretable features. </figcaption> </figure> </div> <h4 id="key-observations-1">Key observations</h4> <ul> <li>two of the most important extracted features by the model were track length and angle of the track</li> <li>by using the pretrained model, we were able to improve the separation of protons and electrons on reference spectrum from reported 94.79% to our 96.2%</li> <li>we have shown that most of the errors are caused by lack of information (tracks smaller than 8 pixels), after filtering those we achieved &gt;99% accuracy on a balanced dataset</li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Tomáš Čelko. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: January 09, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A few projects which I implemented - varying from smaller to larger ones.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in &#39;_pages/cv.md&#39;. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-teaching",title:"teaching",description:"A short history of the courses I taught.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"projects-audio-visualizer",title:"audio_visualizer",description:"A tool to play .wav tracks and visualize fast fourier transform decomposition of the played tune.",section:"Projects",handler:()=>{window.location.href="/projects/audio_visualizer/"}},{id:"projects-chess",title:"Chess",description:"Chess with a simple GUI, and AI oponent based on minimax with alpha-beta prunning algorithm",section:"Projects",handler:()=>{window.location.href="/projects/chess_game/"}},{id:"projects-clusterer-lite",title:"clusterer_lite",description:"Liteweight implementation of clustering targeted at on-chip data processing.",section:"Projects",handler:()=>{window.location.href="/projects/clusterer_lite/"}},{id:"projects-pretraining-of-deep-cnn-autoencoders",title:"Pretraining of deep CNN autoencoders",description:"Autoencoders pretrained on unlabeled data from Timepix3 to improve accuracy.",section:"Projects",handler:()=>{window.location.href="/projects/cnn_autoencoders_pretraining/"}},{id:"projects-comparison-of-dimensionality-reduction-techniques",title:"Comparison of dimensionality reduction techniques",description:"Application of pretrained NN combined with dimensionality reduction for visualization and classification of image data.",section:"Projects",handler:()=>{window.location.href="/projects/dimensionality_reduction/"}},{id:"projects-neat-coevolution",title:"Neat-coevolution",description:"Evolving neural networks using NEAT in co-evolution environment",section:"Projects",handler:()=>{window.location.href="/projects/neat_coevolution/"}},{id:"projects-parallel-clustering-library",title:"Parallel clustering library",description:"Fast connected component analysis for hybrid pixel detectors",section:"Projects",handler:()=>{window.location.href="/projects/parallel_clustering/"}},{id:"projects-pixel-auto",title:"pixel_auto",description:"Development of methodology for application of finite automatons to particle classification",section:"Projects",handler:()=>{window.location.href="/projects/pixel_auto/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%79%6F%75@%65%78%61%6D%70%6C%65.%63%6F%6D","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0009-0003-3077-3223","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>